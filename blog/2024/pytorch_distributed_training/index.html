<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Distributed Training with Pytorch | Hung-Yueh Chiang</title> <meta name="author" content="Hung-Yueh Chiang"> <meta name="description" content="Distributed Training with Pytorch"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ut_shield.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="/blog/2024/pytorch_distributed_training/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hung-Yueh </span>Chiang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Distributed Training with Pytorch</h1> <p class="post-meta">June 30, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/linux"> <i class="fas fa-hashtag fa-sm"></i> linux,</a>   <a href="/blog/tag/ubuntu"> <i class="fas fa-hashtag fa-sm"></i> ubuntu,</a>   <a href="/blog/tag/nvidia"> <i class="fas fa-hashtag fa-sm"></i> nvidia,</a>   <a href="/blog/tag/commands"> <i class="fas fa-hashtag fa-sm"></i> commands,</a>   <a href="/blog/tag/coding"> <i class="fas fa-hashtag fa-sm"></i> coding,</a>   <a href="/blog/tag/pytorch"> <i class="fas fa-hashtag fa-sm"></i> pytorch</a>   </p> </header> <article class="post-content"> <h1 id="introduction">Introduction</h1> <p>Distributed training is necessary for large models training tasks such as neural architecture search supernet, diffusion model or large language models. This document contains the steps to configure machines and launch distributed training tasks with Pytorch framework on GPU servers.</p> <p><br></p> <h1 id="server-configuration">Server Configuration</h1> <h4 id="step-1-disable-firewall-and-ipv6">Step 1 Disable firewall and IPV6</h4> <blockquote> <p>[WARNING] This step may expose the machines under certain threats and attacks.</p> </blockquote> <p>Disable firewall</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ufw status
Status: active
<span class="nv">$ </span><span class="nb">sudo </span>ufw disable
<span class="nv">$ </span><span class="nb">sudo </span>ufw status
Status: inactive
</code></pre></div></div> <p>Disable IPV6:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>sysctl <span class="nt">-w</span> net.ipv6.conf.all.disable_ipv6<span class="o">=</span>1
<span class="nv">$ </span><span class="nb">sudo </span>sysctl <span class="nt">-w</span> net.ipv6.conf.default.disable_ipv6<span class="o">=</span>1
</code></pre></div></div> <h4 id="step-2-export-nccl-socket">Step 2. Export NCCL SOCKET</h4> <p>Some of our machines do not use the default <strong>eth0</strong>, so we have to configure the <code class="language-plaintext highlighter-rouge">NCCL_SOCKET_IFNAME</code> correctly on <strong>all</strong> the machines.</p> <p>To check the socket name.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ifconfig     
docker0: <span class="nv">flags</span><span class="o">=</span>4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500                            
        inet xx.xx.xx.xx  netmask 255.255.0.0  broadcast xx.xx.xx.xx          
        ether 02:42:fe:49:77:fe  txqueuelen 0  <span class="o">(</span>Ethernet<span class="o">)</span>                        
        RX packets 0  bytes 0 <span class="o">(</span>0.0 B<span class="o">)</span>                                            
        RX errors 0  dropped 0  overruns 0  frame 0                              
        TX packets 0  bytes 0 <span class="o">(</span>0.0 B<span class="o">)</span>                                            
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0               
                                                                                 
eno1: <span class="nv">flags</span><span class="o">=</span>4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500                       
        inet xx.xx.123.214  netmask 255.255.255.192  broadcast xx.xx.xx.xx   
        ether 3c:ec:ef:05:1f:ca  txqueuelen 1000  <span class="o">(</span>Ethernet<span class="o">)</span>                     
        RX packets 478564990  bytes 700182821494 <span class="o">(</span>700.1 GB<span class="o">)</span>                      
        RX errors 0  dropped 6  overruns 2135  frame 0                           
        TX packets 880702  bytes 274251519 <span class="o">(</span>274.2 MB<span class="o">)</span>                            
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0               
        device memory 0xc1320000-c133ffff                                        
                                                                                 
eno2: <span class="nv">flags</span><span class="o">=</span>4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500                       
        inet xx.xx.123.215  netmask 255.255.255.192  broadcast xx.xx.xx.xx  
        ether 3c:ec:ef:05:1f:cb  txqueuelen 1000  <span class="o">(</span>Ethernet<span class="o">)</span>                     
        RX packets 136141633  bytes 187956572012 <span class="o">(</span>187.9 GB<span class="o">)</span>                      
        RX errors 0  dropped 6  overruns 2335  frame 0                           
        TX packets 178699579  bytes 168433035752 <span class="o">(</span>168.4 GB<span class="o">)</span>                      
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0               
        device memory 0xc1300000-c131ffff         
</code></pre></div></div> <p>Install <code class="language-plaintext highlighter-rouge">net-tools</code> if you see the error</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Command <span class="s1">'ifconfig'</span> not found, but can be installed with:
apt <span class="nb">install </span>net-tools
Please ask your administrator.
</code></pre></div></div> <p>Export the corresponding socket name</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Both eno1 and eno2 should work.</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>eno2
</code></pre></div></div> <p><br></p> <h4 id="step-3-launch-distributed-training">Step 3. Launch Distributed Training</h4> <p>Get the IP addresses of the machines in the cluster. Note that the ip address should match the NCCL_SOCKET_IFNAME that we export. For example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lovelace: xx.xx.120.112
hopper: xx.xx.121.104
borg: xx.xx.122.109
johnson: xx.xx.123.214
allen: xx.xx.124.214
</code></pre></div></div> <p><br></p> <h4 id="step-4-run-training-scripts">Step 4. Run training scripts</h4> <p>Lei Mao wrote a good <a href="https://leimao.github.io/blog/PyTorch-Distributed-Training/" rel="external nofollow noopener" target="_blank">toy example</a> where we could start with. Here we use 2 machines (–nnodes) as an example. In each node, 4 GPUs (–nproc-per-node) will be used. Therefore, the world size is 8 (8 GPUs in total).</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Host: (--node-rank 0)</span>
torchrun <span class="nt">--nnodes</span><span class="o">=</span>2 <span class="nt">--nproc-per-node</span><span class="o">=</span>4 <span class="nt">--node-rank</span><span class="o">=</span>0 <span class="se">\</span>
        <span class="nt">--rdzv-id</span><span class="o">=</span>456 <span class="nt">--rdzv-backend</span><span class="o">=</span>c10d   <span class="se">\</span>
        <span class="nt">--rdzv-endpoint</span><span class="o">=</span>xx.xx.120.112:29500 <span class="se">\</span>
        train.py <span class="nt">--param1</span> xxx <span class="nt">--param2</span> yyy …
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Client: (--node-rank 1)</span>
torchrun <span class="nt">--nnodes</span><span class="o">=</span>2 <span class="nt">--nproc-per-node</span><span class="o">=</span>4 <span class="nt">--node-rank</span><span class="o">=</span>1 <span class="se">\</span>
        <span class="nt">--rdzv-id</span><span class="o">=</span>456 <span class="nt">--rdzv-backend</span><span class="o">=</span>c10d   <span class="se">\</span>
        <span class="nt">--rdzv-endpoint</span><span class="o">=</span>xx.xx.120.112:29500 <span class="se">\ </span>       
        train.py <span class="nt">--param1</span> xxx <span class="nt">--param2</span> yyy …

</code></pre></div></div> <p><br></p> <h1 id="network-bandwidth">Network Bandwidth</h1> <p>Pytorch relies on the Internet connection to pass the updated weights. Therefore, a high bandwidth machine that serves as the master will speed up the training.</p> <p>Test Bandwidth without sudo</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl <span class="nt">-s</span> https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py | python -
...
...
Testing download speed................................................................................
Download: 88.85 Mbit/s
Testing upload speed......................................................................................................
Upload: 11.87 Mbit/s
</code></pre></div></div> <p>Test Bandwidth with sudo privilege With sudo privilege, we can test a specific network interface, for example enp194s0f0</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool enp194s0f0 | <span class="nb">grep</span> <span class="nt">-i</span> speed
        Speed: 10000Mb/s
</code></pre></div></div> <p><br></p> <h4 id="bandwidth-monitoring-tools">Bandwidth Monitoring Tools</h4> <p>Bmon is a good internet bandwidth monitoring tool. Use the command: <code class="language-plaintext highlighter-rouge">sudo apt install bmon</code> to install the tool. More tools are available here.</p> <p>Here is a snapshot of the distributed training. The weights are passing through the enp194s0f1.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/pytorch_distributed_training/bandwidth-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/pytorch_distributed_training/bandwidth-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/pytorch_distributed_training/bandwidth-1400.webp"></source> <img src="/assets/img/posts/pytorch_distributed_training/bandwidth.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><br></p> <h1 id="troubleshooting">Troubleshooting</h1> <p><br></p> <h4 id="hostname-not-known-issue">Hostname not known <a href="https://github.com/pytorch/pytorch/issues/74824#issuecomment-1500144250" rel="external nofollow noopener" target="_blank">Issue</a> </h4> <p>Error message:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>W socket.cpp:601] <span class="o">[</span>c10d] The IPv6 network addresses of <span class="o">(</span>ece-a58489.austin.utexas.edu, 44685<span class="o">)</span> cannot be retrieved <span class="o">(</span>gai error: <span class="nt">-2</span> - Name or service not known<span class="o">)</span><span class="nb">.</span>
</code></pre></div></div> <p>Solution: add hostname and ip to /etc/hosts. It requires sudo privileges.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /etc/hosts
127.0.0.1 localhost
127.0.1.1 ece-a55028
xx.xx.120.112 ece-a58489.yyy.zzz <span class="c"># add ip for lovelace</span>

<span class="c"># The following lines are desirable for IPv6 capable hosts</span>
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
</code></pre></div></div> <p><br></p> <h4 id="debugging">Debugging</h4> <p>To dump more information for debugging the distributed training, we have to export a few environment variables.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO
<span class="nv">$ </span><span class="nb">export </span><span class="nv">TORCH_CPP_LOG_LEVEL</span><span class="o">=</span>INFO
<span class="nv">$ </span><span class="nb">export </span><span class="nv">TORCH_DISTRIBUTED_DEBUG</span><span class="o">=</span>INFO
</code></pre></div></div> <p>Debugging message:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
master_addr is only used <span class="k">for </span>static rdzv_backend and when rdzv_endpoint is not specified.                                                                                     
WARNING:torch.distributed.run:                                                                                                                                                
<span class="k">*****************************************</span>                                                                                                                                     
Setting OMP_NUM_THREADS environment variable <span class="k">for </span>each process to be 1 <span class="k">in </span>default, to avoid your system being overloaded, please further tune the variable <span class="k">for </span>optimal performa
nce <span class="k">in </span>your application as needed.                                                                                                                                            
<span class="k">*****************************************</span>                                                                                                                                     
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>10.157.244.213, 29500<span class="o">)</span><span class="nb">.</span>                                                    
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:29500 on <span class="o">[</span>johnson.ece.utexas.edu]:46672.                                                  
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>10.157.244.213, 29500<span class="o">)</span><span class="nb">.</span>                                                    
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:29500 on <span class="o">[</span>johnson.ece.utexas.edu]:46682.                                                  
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
<span class="o">[</span>I debug.cpp:49] <span class="o">[</span>c10d] The debug level is <span class="nb">set </span>to INFO.                                                                                                                       
2023-10-09 23:39:49.119410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions <span class="k">in </span>performance-critical
 operations.                                                                                                                                                                  
To <span class="nb">enable </span>the following instructions: AVX2 FMA, <span class="k">in </span>other operations, rebuild TensorFlow with the appropriate compiler flags.                                                  
2023-10-09 23:39:49.170222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions <span class="k">in </span>performance-critical
 operations.                                                                                                                                                                  
To <span class="nb">enable </span>the following instructions: AVX2 FMA, <span class="k">in </span>other operations, rebuild TensorFlow with the appropriate compiler flags.                                                  
2023-10-09 23:39:49.170405: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions <span class="k">in </span>performance-critical
 operations.                                                                                                                                                                  
To <span class="nb">enable </span>the following instructions: AVX2 FMA, <span class="k">in </span>other operations, rebuild TensorFlow with the appropriate compiler flags.                                                  
2023-10-09 23:39:49.170545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions <span class="k">in </span>performance-critical
 operations.                                                                                                                                                                  
To <span class="nb">enable </span>the following instructions: AVX2 FMA, <span class="k">in </span>other operations, rebuild TensorFlow with the appropriate compiler flags.                                                  
2023-10-09 23:39:49.851032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT                                         <span class="o">[</span>488/1839]
2023-10-09 23:39:49.855787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:39:49.965667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
| distributed init <span class="o">(</span>world_size 24, global rank 20, <span class="nb">local </span>gpu <span class="nb">id </span>4<span class="o">)</span>: <span class="nb">env</span>://
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>allen.local, 32927<span class="o">)</span><span class="nb">.</span>
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:32927 on <span class="o">[</span>johnson.ece.utexas.edu]:35816.
| distributed init <span class="o">(</span>world_size 24, global rank 18, <span class="nb">local </span>gpu <span class="nb">id </span>2<span class="o">)</span>: <span class="nb">env</span>://
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>allen.local, 32927<span class="o">)</span><span class="nb">.</span>
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:32927 on <span class="o">[</span>johnson.ece.utexas.edu]:35822.
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>allen.local, 32927<span class="o">)</span><span class="nb">.</span>
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:32927 on <span class="o">[</span>johnson.ece.utexas.edu]:35834.
<span class="o">[</span>I ProcessGroupNCCL.cpp:665] <span class="o">[</span>Rank 20] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 1
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT<span class="o">(</span>ms<span class="o">)</span>: 1800000
USE_HIGH_PRIORITY_STREAM: 0
<span class="o">[</span>I ProcessGroupNCCL.cpp:842] <span class="o">[</span>Rank 20] NCCL watchdog thread started!
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>allen.local, 32927<span class="o">)</span><span class="nb">.</span>
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:32927 on <span class="o">[</span>johnson.ece.utexas.edu]:35840.
<span class="o">[</span>I ProcessGroupNCCL.cpp:665] <span class="o">[</span>Rank 18] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 1
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT<span class="o">(</span>ms<span class="o">)</span>: 1800000
USE_HIGH_PRIORITY_STREAM: 0
<span class="o">[</span>I ProcessGroupNCCL.cpp:842] <span class="o">[</span>Rank 18] NCCL watchdog thread started!
| distributed init <span class="o">(</span>world_size 24, global rank 21, <span class="nb">local </span>gpu <span class="nb">id </span>5<span class="o">)</span>: <span class="nb">env</span>://
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>allen.local, 32927<span class="o">)</span><span class="nb">.</span>
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:32927 on <span class="o">[</span>johnson.ece.utexas.edu]:35852.
<span class="o">[</span>I socket.cpp:624] <span class="o">[</span>c10d - debug] The client socket will attempt to connect to an IPv6 address of <span class="o">(</span>allen.local, 32927<span class="o">)</span>./ 
<span class="o">[</span>I socket.cpp:787] <span class="o">[</span>c10d] The client socket has connected to <span class="o">[</span>allen.ece.utexas.edu]:32927 on <span class="o">[</span>johnson.ece.utexas.edu]:35858.
<span class="o">[</span>I ProcessGroupNCCL.cpp:665] <span class="o">[</span>Rank 21] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 1
</code></pre></div></div> <p><br></p> <h1 id="reference">Reference:</h1> <ul> <li><a href="https://leimao.github.io/blog/PyTorch-Distributed-Training/" rel="external nofollow noopener" target="_blank">https://leimao.github.io/blog/PyTorch-Distributed-Training/</a></li> <li><a href="https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html" rel="external nofollow noopener" target="_blank">https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html</a></li> <li><a href="https://github.com/NVIDIA/nccl/issues/833" rel="external nofollow noopener" target="_blank">https://github.com/NVIDIA/nccl/issues/833</a></li> <li><a href="https://github.com/pytorch/pytorch/issues/79388" rel="external nofollow noopener" target="_blank">https://github.com/pytorch/pytorch/issues/79388</a></li> <li><a href="https://github.com/pytorch/pytorch/issues/74824#issuecomment-1500144250" rel="external nofollow noopener" target="_blank">https://github.com/pytorch/pytorch/issues/74824#issuecomment-1500144250</a></li> <li><a href="https://linuxconfig.org/how-to-enable-disable-firewall-on-ubuntu-20-04-lts-focal-fossa-linux" rel="external nofollow noopener" target="_blank">https://linuxconfig.org/how-to-enable-disable-firewall-on-ubuntu-20-04-lts-focal-fossa-linux</a></li> <li><a href="https://askubuntu.com/questions/257263/how-to-display-network-traffic-in-the-terminal" rel="external nofollow noopener" target="_blank">https://askubuntu.com/questions/257263/how-to-display-network-traffic-in-the-terminal</a></li> </ul> </article><div id="disqus_thread" style="max-width: 1000px; margin: 0 auto"></div> <script type="text/javascript">var disqus_shortname="https-hychiang-info",disqus_identifier="/blog/2024/pytorch_distributed_training",disqus_title="Distributed Training with Pytorch";!function(){var t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(t)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=190&amp;t=tt&amp;d=63L-AOUUXV7dNP9sE0J35-A9m9tVx4XT7qV2CT9AAC8&amp;co=2d78ad&amp;cmo=3acc3a&amp;cmn=ff5353&amp;ct=ffffff"></script> <div class="container"> © Copyright 2024 Hung-Yueh Chiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>